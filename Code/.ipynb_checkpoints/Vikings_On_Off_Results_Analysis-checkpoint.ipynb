{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option(\"display.precision\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for solver in [\"minion\", \"kissat\", \"kissat_mdd\", \"chuffed\"]:\n",
    "    for type_folder in [\"/leave_one_out_custom\", \"/single_problem_custom\", \"/instance_custom\"]:\n",
    "        for subset_tmp in [\"rf_subset\", \"dt_all\", \"rf_all\", \"dt_subset\"]:\n",
    "            \n",
    "            dir_logs = \"../ML_logs/\"+subset_tmp+\"/\"+solver+type_folder\n",
    "            files = os.listdir(dir_logs)\n",
    "\n",
    "            ground_truth = \"../Dataset/results_on_off_\"+solver+\".csv\"\n",
    "            result_df = pd.read_csv(ground_truth)\n",
    "            result_df.drop_duplicates([\"Problem\", \"Policy\"], keep=\"first\", inplace=True)\n",
    "            max_time = result_df['SolverTotalTime'].max() + result_df['SavileRowTotalTime'].max()\n",
    "            \n",
    "            targets = dict()\n",
    "            tot_time_without_tab = 0.0\n",
    "            tot_time_with_tab = 0.0\n",
    "            for prob in result_df[\"Problem\"].unique():\n",
    "                baseline = result_df.query(\"Problem=='\"+prob+\"' and Policy=='baseline'\")\n",
    "                if len(baseline) == 0:\n",
    "                    continue\n",
    "                baseline_time = baseline['SolverTotalTime'].values[0] + baseline['SavileRowTotalTime'].values[0]\n",
    "                tab2 = result_df.query(\"Problem=='\"+prob+\"' and Policy=='2'\")\n",
    "                if len(tab2) == 0:\n",
    "                    continue\n",
    "                tab2_time = tab2['SolverTotalTime'].values[0] + tab2['SavileRowTotalTime'].values[0]\n",
    "                baselineTimeOut = baseline[\"SolverTimeOut\"].fillna(1).values+baseline[\"SavileRowTimeOut\"].fillna(1).values\n",
    "                tab2TimeOut = tab2[\"SolverTimeOut\"].fillna(1).values+tab2[\"SavileRowTimeOut\"].fillna(1).values\n",
    "                if baselineTimeOut>0 and tab2TimeOut>0:\n",
    "                    continue  # drop row\n",
    "                if baselineTimeOut>0 and tab2TimeOut==0:\n",
    "                    clipped = np.clip(tab2_time, 0, 3600)\n",
    "                    targets[prob] = {'Target': 1, 'Score': 3600-clipped, 'Tab_t': clipped, 'Base_t': 3600}\n",
    "                elif baselineTimeOut==0 and tab2TimeOut>0:\n",
    "                    clipped = np.clip(baseline_time, 0, 3600)\n",
    "                    targets[prob] = {'Target': 0, 'Score': clipped-3600, 'Tab_t': 3600, 'Base_t': clipped}\n",
    "                elif baselineTimeOut==0 and tab2TimeOut==0 and np.abs(tab2_time-baseline_time)<1:\n",
    "                    continue # Skip small differences\n",
    "                elif baselineTimeOut==0 and tab2TimeOut==0:# and np.abs(tab2_time-baseline_time)>=1:\n",
    "                    label = 1 if tab2_time < baseline_time else 0\n",
    "                    clipped_1 = np.clip(tab2_time, 0, 3600)\n",
    "                    clipped_2 = np.clip(baseline_time, 0, 3600)\n",
    "                    targets[prob] = {'Target': label, 'Score': clipped_2-clipped_1, 'Tab_t': clipped_1, 'Base_t': clipped_2}\n",
    "            target_df = pd.DataFrame.from_dict(targets, orient ='index')\n",
    "            target_df['Problem'] = target_df.index\n",
    "            target_df['similar'] = target_df.apply(lambda row: 1 if (row['Tab_t']<=60 and row['Base_t']<=60) or np.abs(row['Tab_t'] - row['Base_t']) < 0.1*min([row['Base_t'], row['Tab_t']]) else 0, axis=1)\n",
    "            name = dir_logs.split(\"/\")[-1].split(\"_\")[0]\n",
    "            df = pd.DataFrame()\n",
    "            if (name == \"instance\" or name == \"problem\"):\n",
    "                problems = []\n",
    "                for file in files:\n",
    "                    filename = os.path.join(dir_logs, file)\n",
    "                    with open(filename, \"rb\") as pkl_f:\n",
    "                        test_problems, train_problems, all_test, all_train = pickle.load(pkl_f)\n",
    "                    problems.append([test_problems, train_problems])\n",
    "                    df = pd.concat([df, pd.DataFrame(all_test)])\n",
    "                    df = pd.concat([df, pd.DataFrame(all_train)])\n",
    "                df = df.reset_index().drop([\"index\"], axis=1)\n",
    "            elif name == \"single\":\n",
    "                for file in files:\n",
    "                    complete_df = pd.DataFrame()\n",
    "                    filename = os.path.join(dir_logs, file)\n",
    "                    with open(filename, \"rb\") as pkl_f:\n",
    "                        all_test, all_train = pickle.load(pkl_f)\n",
    "                    df = pd.concat([df, pd.DataFrame(all_test)])\n",
    "                    df = pd.concat([df, pd.DataFrame(all_train)])\n",
    "                    df.reset_index().drop([\"index\"], axis=1)\n",
    "            elif name == \"leave\":\n",
    "                complete_df = pd.DataFrame()\n",
    "                for file in files:\n",
    "                    filename = os.path.join(dir_logs, file)\n",
    "                    with open(filename, \"rb\") as pkl_f:\n",
    "                        all_test = pickle.load(pkl_f)[0]\n",
    "                    complete_df = pd.concat([complete_df, pd.DataFrame(all_test)])\n",
    "                    #complete_df = pd.concat([complete_df, pd.DataFrame(all_train)])\n",
    "                df = complete_df.reset_index().drop([\"index\"], axis=1)\n",
    "                    \n",
    "            if name == \"instance\" or name == \"problem\":\n",
    "                complete_df = df.copy()\n",
    "                df_new = pd.DataFrame()\n",
    "                num_cols = [x for x in complete_df.columns if x not in [\"Problem\", \"train_test\", \"prediction\"]]\n",
    "                for prob in complete_df['Problem'].unique():\n",
    "                    prob_df = complete_df.query(\"Problem=='\"+prob+\"'\")\n",
    "                    for train_test in prob_df['train_test'].unique():\n",
    "                        a = prob_df.query(\"train_test=='\"+train_test+\"'\")\n",
    "                        tmp_df = a[num_cols].mean().to_frame().T\n",
    "                        tmp_df[\"Problem\"] = prob\n",
    "                        tmp_df[\"train_test\"] = train_test\n",
    "                        tmp_df[\"prediction\"] = 1 if (a[\"prediction\"]==a[\"Problem\"]).sum()>=len(a[\"Problem\"])/2 else 0\n",
    "                        df_new = pd.concat([df_new, tmp_df])\n",
    "                df = df_new.reset_index().drop([\"index\"], axis=1)\n",
    "            elif name == \"single\" or name==\"leave\":\n",
    "                df = df.groupby([\"Problem\", \"train_test\"]).agg({\n",
    "                    \"saved_v_best\": 'mean',\n",
    "                    \"saved_class\": 'mean',\n",
    "                    \"saved_heur\": 'mean',\n",
    "                    \"tot_best\": 'mean',\n",
    "                    \"tot_class\": 'mean',\n",
    "                    \"tot_heur\": 'mean',\n",
    "                    \"tot_no_tab\": 'mean',\n",
    "                    #\"prediction\": 'first'\n",
    "                })\n",
    "                df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = \"kissat_mdd\"\n",
    "\n",
    "type_folder = \"/single_problem_custom\"\n",
    "dir_logs = \"../ML_logs/rf_all/\"+solver+type_folder\n",
    "files = os.listdir(dir_logs)\n",
    "\n",
    "ground_truth = \"../Dataset/results_on_off_\"+solver+\".csv\"\n",
    "result_df = pd.read_csv(ground_truth)\n",
    "result_df.drop_duplicates([\"Problem\", \"Policy\"], keep=\"first\", inplace=True)\n",
    "max_time = result_df['SolverTotalTime'].max() + result_df['SavileRowTotalTime'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = dict()\n",
    "tot_time_without_tab = 0.0\n",
    "tot_time_with_tab = 0.0\n",
    "for prob in result_df[\"Problem\"].unique():\n",
    "    baseline = result_df.query(\"Problem=='\"+prob+\"' and Policy=='baseline'\")\n",
    "    if len(baseline) == 0:\n",
    "        continue\n",
    "    baseline_time = baseline['SolverTotalTime'].values[0] + baseline['SavileRowTotalTime'].values[0]\n",
    "    tab2 = result_df.query(\"Problem=='\"+prob+\"' and Policy=='2'\")\n",
    "    if len(tab2) == 0:\n",
    "        continue\n",
    "    tab2_time = tab2['SolverTotalTime'].values[0] + tab2['SavileRowTotalTime'].values[0]\n",
    "    baselineTimeOut = baseline[\"SolverTimeOut\"].fillna(1).values+baseline[\"SavileRowTimeOut\"].fillna(1).values\n",
    "    tab2TimeOut = tab2[\"SolverTimeOut\"].fillna(1).values+tab2[\"SavileRowTimeOut\"].fillna(1).values\n",
    "    if baselineTimeOut>0 and tab2TimeOut>0:\n",
    "        continue  # drop row\n",
    "    if baselineTimeOut>0 and tab2TimeOut==0:\n",
    "        clipped = np.clip(tab2_time, 0, 3600)\n",
    "        targets[prob] = {'Target': 1, 'Score': 3600-clipped, 'Tab_t': clipped, 'Base_t': 3600}\n",
    "    elif baselineTimeOut==0 and tab2TimeOut>0:\n",
    "        clipped = np.clip(baseline_time, 0, 3600)\n",
    "        targets[prob] = {'Target': 0, 'Score': clipped-3600, 'Tab_t': 3600, 'Base_t': clipped}\n",
    "    elif baselineTimeOut==0 and tab2TimeOut==0 and np.abs(tab2_time-baseline_time)<1:\n",
    "        continue # Skip small differences\n",
    "    elif baselineTimeOut==0 and tab2TimeOut==0:\n",
    "        label = 1 if tab2_time < baseline_time else 0\n",
    "        clipped_1 = np.clip(tab2_time, 0, 3600)\n",
    "        clipped_2 = np.clip(baseline_time, 0, 3600)\n",
    "        targets[prob] = {'Target': label, 'Score': clipped_2-clipped_1, 'Tab_t': clipped_1, 'Base_t': clipped_2}\n",
    "target_df = pd.DataFrame.from_dict(targets, orient ='index')\n",
    "target_df['Problem'] = target_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df['similar'] = target_df.apply(lambda row: 1 if (row['Tab_t']<=60 and row['Base_t']<=60) or np.abs(row['Tab_t'] - row['Base_t']) < 0.1*min([row['Base_t'], row['Tab_t']]) else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = dir_logs.split(\"/\")[-1].split(\"_\")[0]\n",
    "df = pd.DataFrame()\n",
    "if (name == \"instance\" or name == \"problem\"):\n",
    "    problems = []\n",
    "    for file in files:\n",
    "        filename = os.path.join(dir_logs, file)\n",
    "        with open(filename, \"rb\") as pkl_f:\n",
    "            test_problems, train_problems, all_test, all_train = pickle.load(pkl_f)\n",
    "        problems.append([test_problems, train_problems])\n",
    "        df = pd.concat([df, pd.DataFrame(all_test)])\n",
    "        df = pd.concat([df, pd.DataFrame(all_train)])\n",
    "    df = df.reset_index().drop([\"index\"], axis=1)\n",
    "elif name == \"single\":\n",
    "    for file in files:\n",
    "        complete_df = pd.DataFrame()\n",
    "        filename = os.path.join(dir_logs, file)\n",
    "        with open(filename, \"rb\") as pkl_f:\n",
    "            all_test, all_train = pickle.load(pkl_f)\n",
    "        df = pd.concat([df, pd.DataFrame(all_test)])\n",
    "        df = pd.concat([df, pd.DataFrame(all_train)])\n",
    "        df.reset_index().drop([\"index\"], axis=1)\n",
    "elif name == \"leave\":\n",
    "    complete_df = pd.DataFrame()\n",
    "    for file in files:\n",
    "        filename = os.path.join(dir_logs, file)\n",
    "        with open(filename, \"rb\") as pkl_f:\n",
    "            all_test = pickle.load(pkl_f)[0]\n",
    "        complete_df = pd.concat([complete_df, pd.DataFrame(all_test)])\n",
    "        #complete_df = pd.concat([complete_df, pd.DataFrame(all_train)])\n",
    "    df = complete_df.reset_index().drop([\"index\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if name == \"instance\" or name == \"problem\":\n",
    "    complete_df = df.copy()\n",
    "    df_new = pd.DataFrame()\n",
    "    df2 = pd.DataFrame()\n",
    "    num_cols = [x for x in complete_df.columns if x not in [\"Problem\", \"train_test\", \"prediction\"]]\n",
    "    for prob in complete_df['Problem'].unique():\n",
    "        prob_df = complete_df.query(\"Problem=='\"+prob+\"'\")\n",
    "        for train_test in prob_df['train_test'].unique():\n",
    "            a = prob_df.query(\"train_test=='\"+train_test+\"'\")\n",
    "            tmp_df = a[num_cols].mean().to_frame().T\n",
    "            tmp_df[\"Problem\"] = prob\n",
    "            tmp_df[\"train_test\"] = train_test\n",
    "            tmp_df[\"prediction\"] = 1 if (a[\"prediction\"]==a[\"Problem\"]).sum()>=len(a[\"Problem\"])/2 else 0\n",
    "            df_new = pd.concat([df_new, tmp_df])\n",
    "    df = df_new.reset_index().drop([\"index\"], axis=1)\n",
    "elif name == \"single\" or name==\"leave\":\n",
    "    df = df.groupby([\"Problem\", \"train_test\"]).agg({\n",
    "        \"saved_v_best\": 'mean',\n",
    "        \"saved_class\": 'mean',\n",
    "        \"saved_heur\": 'mean',\n",
    "        \"tot_best\": 'mean',\n",
    "        \"tot_class\": 'mean',\n",
    "        \"tot_heur\": 'mean',\n",
    "        \"tot_no_tab\": 'mean',\n",
    "        #\"prediction\": 'first'\n",
    "    })\n",
    "    df = df.reset_index()\n",
    "sums = df.query(\"train_test=='test'\").drop([\"Problem\", \"train_test\"], axis = 1).sum()\n",
    "if sums[\"saved_heur\"]>0:\n",
    "    sums[\"gap\"] = 100*(sums[\"tot_class\"]-sums[\"tot_heur\"])/(sums[\"tot_best\"]-sums[\"tot_heur\"])\n",
    "else:\n",
    "    sums[\"gap\"] = 100*(sums[\"tot_class\"]-sums[\"tot_no_tab\"])/(sums[\"tot_best\"]-sums[\"tot_no_tab\"])\n",
    "print(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if name == \"leave\":\n",
    "    a = df.copy()\n",
    "    a[\"Problem\"] = df.apply(lambda x: x[\"Problem\"].split(\"_\")[0], axis=1)\n",
    "    a = a.groupby([\"Problem\", \"train_test\"]).agg(\"mean\", numeric_only=True).reset_index()\n",
    "    a[\"saved_s_best\"] = 0\n",
    "    for prob in a.Problem.unique():\n",
    "        c = a.query(\"train_test=='test' and Problem!='\"+prob+\"'\")\n",
    "        mask_dfs = df[\"Problem\"].str.contains(prob+\"_\") & df[\"train_test\"].str.match(\"test\")\n",
    "        df.loc[mask_dfs, \"saved_s_best\"] = 0 if c[\"saved_heur\"].sum()<=0 else df.loc[mask_dfs, \"saved_heur\"]\n",
    "elif name == \"single\":\n",
    "    df[\"saved_s_best\"] = 0\n",
    "    a = df.copy()\n",
    "    a[\"Problem\"] = df.apply(lambda x: x[\"Problem\"].split(\"_\")[0], axis=1)\n",
    "    a = a.groupby([\"Problem\", \"train_test\"]).agg(\"mean\", numeric_only=True).reset_index()\n",
    "\n",
    "    for prob in a.Problem.unique():\n",
    "        c = a.query(\"train_test=='train' and Problem=='\"+prob+\"'\")\n",
    "        mask = df[\"Problem\"].str.contains(prob) & df[\"train_test\"].str.match(\"train\")\n",
    "        df.loc[mask, \"saved_s_best\"] = 0 if c[\"saved_heur\"].iloc[0]<=0 else df.loc[mask, \"saved_heur\"]\n",
    "        mask = df[\"Problem\"].str.contains(prob) & df[\"train_test\"].str.match(\"test\")\n",
    "        df.loc[mask, \"saved_s_best\"] = 0 if c[\"saved_heur\"].iloc[0]<=0 else df.loc[mask, \"saved_heur\"]\n",
    "\n",
    "if name == \"leave\" or name == \"single\":\n",
    "    x = np.linspace(0, 3600, 36000)\n",
    "\n",
    "    y = {'ML Model': [], 'No Tabulation': [], 'Heuristics': [], 'Single Best': []}\n",
    "\n",
    "    ov_00 = df.query(\"train_test=='test'\")['tot_class']\n",
    "    ov_25 = df.query(\"train_test=='test'\")['tot_no_tab']\n",
    "    ov_50 = df.query(\"train_test=='test'\")['tot_heur']\n",
    "    ov_75 = df.query(\"train_test=='test'\")['tot_no_tab']-df.query(\"train_test=='test'\")['saved_s_best']\n",
    "\n",
    "    for i in x:\n",
    "        y['ML Model'].append((ov_00<i).sum())\n",
    "        y['No Tabulation'].append((ov_25<i).sum())\n",
    "        y['Heuristics'].append((ov_50<i).sum())\n",
    "        y['Single Best'].append((ov_75<i).sum())\n",
    "    plt.figure(figsize=(18, 8));\n",
    "    plt.plot(x, y['ML Model'])\n",
    "    plt.plot(x, y['No Tabulation'])\n",
    "    plt.plot(x, y['Heuristics'])\n",
    "    plt.plot(x, y['Single Best'])\n",
    "    plt.legend(['ML Model', 'No Tabulation', 'Heuristics', 'Single Best'])\n",
    "    plt.title(\"Number of instances solved over time in Per Problem Class with \"+solver);\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"# instances\")\n",
    "    plt.tight_layout();\n",
    "    #plt.xlim(left=25)\n",
    "    #plt.ylim(bottom=120)\n",
    "    #plt.savefig(solver+\"_\"+type_folder.split(\"_\")[0][1:]+\"_cactusplot.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_metrics(tab_act_num=1):\n",
    "    target_df[\"pred\"] = -1\n",
    "    if name in [\"leave\", \"single\"]:\n",
    "        probs_stats = pd.DataFrame()\n",
    "        for _, row in df.query(\"train_test == 'test'\").iterrows():\n",
    "            prob_stats_dict = {\"fp\": [0], \"fp_time\": [0.0], \"fn\": [0], \"fn_time\": [0.0], \n",
    "                               \"no_diff\": [0], \"no_diff_time\": [0.0], \"Problem\": row[\"Problem\"], \n",
    "                               \"tp\": [0], \"tp_time\": [0.0], \"tn\": [0], \"tn_time\": [0.0], \"tp_heur\": [0.0], \n",
    "                               \"fp_heur\": [0.0], \"tn_heur\": [0.0], \"fn_heur\": [0.0], \"disagreement_time\": [0.0], \n",
    "                               \"agreement_time\": [0.0]}\n",
    "            for label_instance in row[\"prediction\"]:\n",
    "                instance = label_instance[4:]\n",
    "                target_df.loc[instance, \"pred\"] = 1 if label_instance[:3]==\"pos\" else 0\n",
    "                stats = target_df.query(\"Problem == '\"+ instance+ \"'\")\n",
    "                if len(stats) <1:\n",
    "                    continue\n",
    "                fp_flag = stats[\"pred\"].values[0]==1 and stats[\"Target\"].values[0] == 0\n",
    "                fn_flag = stats[\"pred\"].values[0]==0 and stats[\"Target\"].values[0] == 1\n",
    "                tp_flag = stats[\"pred\"].values[0]==1 and stats[\"Target\"].values[0] == 1\n",
    "                tn_flag = stats[\"pred\"].values[0]==0 and stats[\"Target\"].values[0] == 0\n",
    "                if tab_act_num != -1:\n",
    "                    tab_activated = result_df.query(\"Problem == '\"+instance+\"'\")[\"TabulationActivated\"+str(tab_act_num)].dropna()\n",
    "                else:\n",
    "                    tab_activated = result_df.query(\"Problem == '\"+instance+\"'\")[\"TabulationActivated\"].dropna()\n",
    "                if len(tab_activated) <1:\n",
    "                    continue\n",
    "                tab_activated = [1 if x>0 else 0 for x in tab_activated]\n",
    "                tp_heur = tab_activated[0] == 1 and stats[\"pred\"].values[0]==1\n",
    "                fp_heur = tab_activated[0] == 0 and stats[\"pred\"].values[0]==1\n",
    "                tn_heur = tab_activated[0] == 0 and stats[\"pred\"].values[0]==0\n",
    "                fn_heur = tab_activated[0] == 1 and stats[\"pred\"].values[0]==0\n",
    "                if fp_flag and stats[\"similar\"].values[0] == 0:\n",
    "                    prob_stats_dict[\"fp\"][0] += 1\n",
    "                    prob_stats_dict[\"fp_time\"][0] += np.abs(stats[\"Score\"].values[0])\n",
    "                elif fn_flag and stats[\"similar\"].values[0] == 0:\n",
    "                    prob_stats_dict[\"fn\"][0] += 1\n",
    "                    prob_stats_dict[\"fn_time\"][0] += np.abs(stats[\"Score\"].values[0])\n",
    "                elif (fp_flag or fn_flag) and stats[\"similar\"].values[0] == 1:\n",
    "                    prob_stats_dict[\"no_diff\"][0] += 1\n",
    "                    prob_stats_dict[\"no_diff_time\"][0] += np.abs(stats[\"Score\"].values[0])\n",
    "                elif tp_flag:\n",
    "                    prob_stats_dict[\"tp\"][0] += 1\n",
    "                    prob_stats_dict[\"tp_time\"][0] += np.abs(stats[\"Score\"].values[0])\n",
    "                elif tn_flag:\n",
    "                    prob_stats_dict[\"tn\"][0] += 1\n",
    "                    prob_stats_dict[\"tn_time\"][0] += np.abs(stats[\"Score\"].values[0])\n",
    "                if fp_heur:\n",
    "                    prob_stats_dict[\"fp_heur\"][0] += 1\n",
    "                    prob_stats_dict[\"disagreement_time\"][0] += stats[\"Tab_t\"].values[0]-(stats[\"Base_t\"].values[0]-stats[\"pred\"].values[0]*stats[\"Score\"].values[0])\n",
    "                elif fn_heur:\n",
    "                    prob_stats_dict[\"fn_heur\"][0] += 1\n",
    "                    prob_stats_dict[\"disagreement_time\"][0] += stats[\"Tab_t\"].values[0]-(stats[\"Base_t\"].values[0]-stats[\"pred\"].values[0]*stats[\"Score\"].values[0])\n",
    "                elif tp_heur:\n",
    "                    prob_stats_dict[\"tp_heur\"][0] += 1\n",
    "                    prob_stats_dict[\"agreement_time\"][0] += stats[\"Tab_t\"].values[0]-(stats[\"Base_t\"].values[0]-stats[\"pred\"].values[0]*stats[\"Score\"].values[0])\n",
    "                elif tn_heur:\n",
    "                    prob_stats_dict[\"tn_heur\"][0] += 1\n",
    "                    prob_stats_dict[\"agreement_time\"][0] += stats[\"Tab_t\"].values[0]-(stats[\"Base_t\"].values[0]-stats[\"pred\"].values[0]*stats[\"Score\"].values[0])\n",
    "            prob_stats_dict[\"precision\"]=prob_stats_dict[\"tp\"][0]/(prob_stats_dict[\"tp\"][0]+prob_stats_dict[\"fp\"][0]+0.000001)\n",
    "            prob_stats_dict[\"recall\"]=prob_stats_dict[\"tp\"][0]/(prob_stats_dict[\"tp\"][0]+prob_stats_dict[\"fn\"][0]+0.000001)\n",
    "            prob_stats_dict[\"f1\"]=2*prob_stats_dict[\"recall\"]*prob_stats_dict[\"precision\"]/(prob_stats_dict[\"recall\"]+prob_stats_dict[\"precision\"]+0.000001)\n",
    "            probs_stats = pd.concat([probs_stats, pd.DataFrame.from_dict(prob_stats_dict)])\n",
    "        probs_stats = probs_stats.reset_index()\n",
    "        probs_stats.drop([\"Problem\"], axis=1).describe()\n",
    "    elif name in [\"problem\", \"instance\"]:\n",
    "        probs_stats = pd.DataFrame()\n",
    "        done = []\n",
    "        for _, row in df.query(\"train_test == 'test'\").iterrows():\n",
    "            curr_prob = row[\"Problem\"].split(\"_\")[0]\n",
    "            prob_stats_dict = {\"fp\": [0], \"fp_time\": [0.0], \"fn\": [0], \"fn_time\": [0.0], \n",
    "                               \"no_diff\": [0], \"no_diff_time\": [0.0], \"tp\": [0], \"tp_time\": [0.0],\n",
    "                               \"tn\": [0], \"tn_time\": [0.0], \"tp_heur\": [0.0], \n",
    "                               \"fp_heur\": [0.0], \"tn_heur\": [0.0], \"fn_heur\": [0.0], \"disagreement_time\": [0.0], \n",
    "                               \"agreement_time\": [0.0]}\n",
    "            fp_flag = row[\"saved_v_best\"]==0 and row[\"saved_heur\"]<0 and row[\"saved_class\"] < row[\"saved_heur\"]\n",
    "            fn_flag = row[\"saved_v_best\"]>0 and row[\"saved_heur\"]>0 and row[\"saved_class\"] <= 0\n",
    "            tp_flag = row[\"saved_v_best\"]>0 and row[\"saved_heur\"]>0 and row[\"saved_class\"] > 0\n",
    "            tn_flag = row[\"saved_v_best\"]==0 and row[\"saved_heur\"]<0 and row[\"saved_class\"] <= 0\n",
    "            if tab_act_num == -1:\n",
    "                tab_activated = result_df.query(\"Problem == '\"+row[\"Problem\"]+\"' and Policy=='2'\")[\"TabulationActivated\"].dropna()\n",
    "            else:\n",
    "                tab_activated = result_df.query(\"Problem == '\"+row[\"Problem\"]+\"' and Policy=='2'\")[\"TabulationActivated\"+str(tab_act_num)].dropna()\n",
    "            if len(tab_activated) <1:\n",
    "                continue\n",
    "            tab_activated = [1 if x>0 else 0 for x in tab_activated]\n",
    "            tp_heur = tab_activated[0] == 1 and (tp_flag or fp_flag)\n",
    "            fp_heur = tab_activated[0] == 0 and (tp_flag or fp_flag)\n",
    "            tn_heur = tab_activated[0] == 0 and (tn_flag or fn_flag)\n",
    "            fn_heur = tab_activated[0] == 1 and (tn_flag or fn_flag)\n",
    "            if fp_flag:\n",
    "                prob_stats_dict[\"fp\"][0] += 1\n",
    "            elif fn_flag:\n",
    "                prob_stats_dict[\"fn\"][0] += 1\n",
    "            elif tp_flag:\n",
    "                prob_stats_dict[\"tp\"][0] += 1\n",
    "            elif tn_flag:\n",
    "                prob_stats_dict[\"tn\"][0] += 1\n",
    "            if fp_heur:\n",
    "                prob_stats_dict[\"fp_heur\"][0] += 1\n",
    "                prob_stats_dict[\"disagreement_time\"][0] += row[\"saved_class\"]-row[\"saved_heur\"]\n",
    "            elif fn_heur:\n",
    "                prob_stats_dict[\"fn_heur\"][0] += 1\n",
    "                prob_stats_dict[\"disagreement_time\"][0] += row[\"saved_class\"]-row[\"saved_heur\"]\n",
    "            elif tp_heur:\n",
    "                prob_stats_dict[\"tp_heur\"][0] += 1\n",
    "                prob_stats_dict[\"agreement_time\"][0] += row[\"saved_class\"]-row[\"saved_heur\"]\n",
    "            elif tn_heur:\n",
    "                prob_stats_dict[\"tn_heur\"][0] += 1\n",
    "                prob_stats_dict[\"agreement_time\"][0] += row[\"saved_class\"]-row[\"saved_heur\"]\n",
    "            prob_stats_dict[\"precision\"]=prob_stats_dict[\"tp\"][0]/(prob_stats_dict[\"tp\"][0]+prob_stats_dict[\"fp\"][0]+0.000001)\n",
    "            prob_stats_dict[\"recall\"]=prob_stats_dict[\"tp\"][0]/(prob_stats_dict[\"tp\"][0]+prob_stats_dict[\"fn\"][0]+0.000001)\n",
    "            prob_stats_dict[\"f1\"]=2*prob_stats_dict[\"recall\"]*prob_stats_dict[\"precision\"]/(prob_stats_dict[\"recall\"]+prob_stats_dict[\"precision\"]+0.000001)\n",
    "            probs_stats = pd.concat([probs_stats, pd.DataFrame.from_dict(prob_stats_dict)])\n",
    "        probs_stats = probs_stats.reset_index()\n",
    "    return probs_stats, target_df[target_df[\"pred\"]!=-1].copy()\n",
    "\n",
    "if \"TabulationActivated1\" in result_df.columns:\n",
    "    agreement_dict = dict()\n",
    "    disagreement_dict = dict()\n",
    "    for i in [1, 2, 3, 4, 5]:\n",
    "        probs_stats_tmp, target_df_tmp = get_metrics(tab_act_num=i)\n",
    "        agreement_dict[i] = probs_stats_tmp[\"agreement_time\"].sum()\n",
    "        disagreement_dict[i] = probs_stats_tmp[\"disagreement_time\"].sum()\n",
    "        if i == 1:\n",
    "            probs_stats = probs_stats_tmp\n",
    "            target_df = target_df_tmp\n",
    "else:\n",
    "    probs_stats_tmp, target_df_tmp = get_metrics(tab_act_num=-1)\n",
    "    probs_stats = probs_stats_tmp\n",
    "    target_df = target_df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if name in [\"leave\", \"single\"]:\n",
    "    print(\"Total false positives: \", probs_stats[\"fp\"].sum(), \", Cost of fp: \", np.abs(probs_stats[\"fp_time\"]).sum())\n",
    "    print(\"Total false negatives: \", probs_stats[\"fn\"].sum(), \", Cost of fn: \", np.abs(probs_stats[\"fn_time\"]).sum())\n",
    "    print(\"Total no diff: \", probs_stats[\"no_diff\"].sum(), \", Cost of no_diff: \", np.abs(probs_stats[\"no_diff_time\"]).sum())\n",
    "    print(\"Total true positives: \", probs_stats[\"tp\"].sum(), \", Saved with tp: \", np.abs(probs_stats[\"tp_time\"]).sum())\n",
    "    print(\"Total true negatives: \", probs_stats[\"tn\"].sum(), \", Time of tn: \", np.abs(probs_stats[\"tn_time\"]).sum())\n",
    "    print(\"Mean precision: \", probs_stats[\"precision\"].mean())\n",
    "    print(\"Mean recall: \", probs_stats[\"recall\"].mean())\n",
    "    print(\"Mean f1: \", probs_stats[\"f1\"].mean())\n",
    "    print(\"Agreement heuristics-ML: \", probs_stats[\"tp_heur\"].sum()+probs_stats[\"tn_heur\"].sum())\n",
    "    print(\"Disagreement heuristics-ML: \", probs_stats[\"fn_heur\"].sum()+probs_stats[\"fp_heur\"].sum())\n",
    "    print(\"Disagreement time saved: \", probs_stats[\"disagreement_time\"].sum())\n",
    "    print(\"Agreement time saved: \", probs_stats[\"agreement_time\"].sum())\n",
    "    probs_stats.query(\"fp>0 or fn>0 or no_diff>0\").drop([\"index\"], axis=1).reset_index().drop([\"index\"], axis=1)\n",
    "else:\n",
    "    print(\"Total false positives: \", probs_stats[\"fp\"].sum())\n",
    "    print(\"Total false negatives: \", probs_stats[\"fn\"].sum())\n",
    "    print(\"Total true positives: \", probs_stats[\"tp\"].sum())\n",
    "    print(\"Total true negatives: \", probs_stats[\"tn\"].sum())\n",
    "    print(\"Mean precision: \", probs_stats[\"precision\"].mean())\n",
    "    print(\"Mean recall: \", probs_stats[\"recall\"].mean())\n",
    "    print(\"Mean f1: \", probs_stats[\"f1\"].mean())\n",
    "    print(\"Agreement heuristics-ML: \", probs_stats[\"tp_heur\"].sum()+probs_stats[\"tn_heur\"].sum())\n",
    "    print(\"Disagreement heuristics-ML: \", probs_stats[\"fn_heur\"].sum()+probs_stats[\"fp_heur\"].sum())\n",
    "    print(\"Disagreement time saved: \", probs_stats[\"disagreement_time\"].sum())\n",
    "    print(\"Agreement time saved: \", probs_stats[\"agreement_time\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if name == \"single\" or name==\"leave\":\n",
    "    seconds = 60\n",
    "    titles = {\"saved_class\": \"Proportion of time saved for each problem with random forest classifier\", \n",
    "              \"saved_v_best\": \"Proportion of time saved for each problem with virtual best classifier\"}\n",
    "    saved_thresh = \", when the time saved is more than \"+str(seconds)+\" seconds.\"\n",
    "    total_thresh = \", when the total time is more than \"+str(seconds)+\" seconds.\"\n",
    "    for col in [\"saved_class\", \"saved_v_best\"]:\n",
    "        for ext in [\".\", \"thresh\"]:\n",
    "            if ext == \"thresh\" and col.split(\"_\")[0] == \"saved\":\n",
    "                title = titles[col] + saved_thresh\n",
    "                thresh = seconds\n",
    "            elif ext == \"thresh\" and col.split(\"_\")[0] == \"tot\":\n",
    "                title = titles[col] + total_thresh\n",
    "                thresh = seconds\n",
    "            elif ext == \".\":\n",
    "                title = titles[col] + \".\"\n",
    "                thresh = 0\n",
    "            a = df.query(\"train_test == 'test'\")[[\"Problem\", col, \"tot_heur\", \"tot_no_tab\"]]\n",
    "            a = a[a[col]>thresh]\n",
    "            b = a[\"tot_heur\"]  # no_tab\"]\n",
    "            a[col] = b-(a[\"tot_no_tab\"]-a[col])  # saved with respect to the heuristics\n",
    "            a = a[[\"Problem\", col]]\n",
    "            plt.figure(figsize=(20, 6))\n",
    "            plt.bar(np.arange(0, len(a[\"Problem\"]), 1), a[col]/b*100)\n",
    "            plt.title(title)\n",
    "            plt.xlabel(\"Problem\")\n",
    "            plt.ylabel(\"Proportion of time saved with respect to always using the heuristics.\")  # the baseline.\")\n",
    "            plt.xticks(ticks=np.arange(0, len(a[\"Problem\"]), 1), labels=a[\"Problem\"]);\n",
    "            #plt.savefig(col+\"_\"+ext+\"_heur.png\", bbox_inches = 'tight')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if name == \"single\" or name==\"leave\":\n",
    "    seconds = 60\n",
    "    titles = {\"saved_class\": \"Proportion of time saved for each problem with random forest classifier\", \n",
    "              \"saved_v_best\": \"Proportion of time saved for each problem with virtual best classifier\"}\n",
    "    saved_thresh = \", when the time saved is more than \"+str(seconds)+\" seconds.\"\n",
    "    total_thresh = \", when the total time is more than \"+str(seconds)+\" seconds.\"\n",
    "    for col in [\"saved_class\", \"saved_v_best\"]:\n",
    "        for ext in [\".\", \"thresh\"]:\n",
    "            if ext == \"thresh\" and col.split(\"_\")[0] == \"saved\":\n",
    "                title = titles[col] + saved_thresh\n",
    "                thresh = seconds\n",
    "            elif ext == \"thresh\" and col.split(\"_\")[0] == \"tot\":\n",
    "                title = titles[col] + total_thresh\n",
    "                thresh = seconds\n",
    "            elif ext == \".\":\n",
    "                title = titles[col] + \".\"\n",
    "                thresh = 0\n",
    "            a = df.query(\"train_test == 'test'\")[[\"Problem\", col, \"tot_no_tab\"]]\n",
    "            if thresh == 0:\n",
    "                a = a[abs(a[col])>1000]\n",
    "            else:\n",
    "                a = a[a[col]>thresh]\n",
    "            b = a[\"tot_no_tab\"]\n",
    "            a = a[[\"Problem\", col]]\n",
    "            plt.figure(figsize=(20, 6))\n",
    "            plt.bar(np.arange(0, len(a[\"Problem\"]), 1), a[col]/b*100)\n",
    "            plt.title(title)\n",
    "            plt.xlabel(\"Problem\")\n",
    "            plt.ylabel(\"Proportion of time saved with respect to always using the baseline.\")\n",
    "            plt.xticks(ticks=np.arange(0, len(a[\"Problem\"]), 1), labels=a[\"Problem\"]);\n",
    "            #plt.savefig(col+\"_\"+ext+\"_base.png\", bbox_inches = 'tight')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if name == \"single\" or name==\"leave\":\n",
    "    seconds = 60\n",
    "    titles = {\"tot_heur\": \"Total time for each problem when always using the heuristics\", \n",
    "              \"tot_no_tab\": \"Total time for each problem without tabulation.\"}\n",
    "    saved_thresh = \", when the time saved is more than \"+str(seconds)+\" seconds.\"\n",
    "    total_thresh = \", when the total time is more than \"+str(seconds)+\" seconds.\"\n",
    "    for col in [\"tot_heur\", \"tot_no_tab\"]:\n",
    "        for ext in [\".\", \"thresh\"]:\n",
    "            if ext == \"thresh\" and col.split(\"_\")[0] == \"saved\":\n",
    "                title = titles[col] + saved_thresh\n",
    "                thresh = seconds\n",
    "            elif ext == \"thresh\" and col.split(\"_\")[0] == \"tot\":\n",
    "                title = titles[col] + total_thresh\n",
    "                thresh = seconds\n",
    "            elif ext == \".\":\n",
    "                title = titles[col] + \".\"\n",
    "                thresh = 0\n",
    "            a = df.query(\"train_test == 'test'\")[[\"Problem\", col]]\n",
    "            a = a[a[col]>thresh]\n",
    "            plt.figure(figsize=(20, 6))\n",
    "            plt.bar(np.arange(0, len(a[\"Problem\"]), 1), np.log(a[col]))\n",
    "            plt.title(title)\n",
    "            plt.xlabel(\"Problem\")\n",
    "            plt.ylabel(\"Time (s), logaritmic scale\")\n",
    "            plt.xticks(ticks=np.arange(0, len(a[\"Problem\"]), 1), labels=a[\"Problem\"]);\n",
    "            #plt.savefig(col+\"_\"+ext+\".png\", bbox_inches = 'tight')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = df.query(\"train_test=='test'\").drop([\"Problem\", \"train_test\"], axis = 1).sum()\n",
    "b = a[['saved_v_best', 'saved_class', 'saved_heur']]\n",
    "c = a[['tot_best', 'tot_class', 'tot_heur', 'tot_no_tab']]\n",
    "b.plot(kind='bar')\n",
    "plt.title(\"Total time saved with solver: \" + solver[0].upper()+solver[1:])\n",
    "plt.xticks(ticks=np.arange(0,len(b),1), labels=['saved_v_best', 'saved_class', 'saved_heur'])\n",
    "#plt.savefig(\"time_saved.png\", bbox_inches = 'tight')\n",
    "plt.show()\n",
    "c.plot(kind='bar')\n",
    "plt.title(\"Total time with solver: \" + solver[0].upper()+solver[1:])\n",
    "plt.xticks(ticks=np.arange(0,len(c),1), labels=['tot_best', 'tot_class', 'tot_heur', 'tot_no_tab'])\n",
    "#plt.savefig(\"time_total.png\", bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8));\n",
    "ax = plt.gca()\n",
    "if name == \"leave\" or name == \"single\":\n",
    "    max_time = int(max(df.query(\"train_test=='test'\")['tot_best'].max(), df.query(\"train_test=='test'\")['tot_heur'].max()))\n",
    "ax.scatter(df.query(\"train_test=='test'\")['tot_heur'], df.query(\"train_test=='test'\")['tot_best'], c=\"black\", marker=\"x\");\n",
    "ax.set_yscale('symlog')\n",
    "ax.set_xscale('symlog')\n",
    "if name == \"leave\" or name == \"single\":\n",
    "    plt.plot([0, max_time+5], [0, max_time+5], c=\"black\");\n",
    "else:\n",
    "    plt.plot([0, 4800+5], [0, 4800+5], c=\"black\");\n",
    "plt.title(\"Total time for problems in test set\");\n",
    "plt.ylabel(\"Total time of virtual best classifier (s)\");\n",
    "plt.xlabel(\"Total time when always using the heuristics (s)\");\n",
    "plt.tight_layout();\n",
    "if name == \"leave\" or name == \"single\":\n",
    "    plt.xlim(left=0, right=max_time);\n",
    "    plt.ylim(bottom=0, top=max_time);\n",
    "else:\n",
    "    plt.xlim(left=0, right=4800);\n",
    "    plt.ylim(bottom=0, top=4800);\n",
    "#plt.savefig(\"best_heuristics.jpg\", bbox_inches = 'tight')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8));\n",
    "ax = plt.gca()\n",
    "if name == \"leave\" or name == \"single\":\n",
    "    max_time = int(max(df.query(\"train_test=='test'\")['tot_heur'].max(), df.query(\"train_test=='test'\")['tot_class'].max()))\n",
    "ax.scatter(df.query(\"train_test=='test'\")['tot_heur'], df.query(\"train_test=='test'\")['tot_class'], c=\"black\", marker=\"x\");\n",
    "ax.set_yscale('symlog')\n",
    "ax.set_xscale('symlog')\n",
    "if name == \"leave\" or name == \"single\":\n",
    "    plt.plot([0, max_time+5], [0, max_time+5], c=\"black\");\n",
    "else:\n",
    "    plt.plot([0, 4800+5], [0, 4800+5], c=\"black\");\n",
    "plt.title(\"Total time for problems in test set\");\n",
    "plt.ylabel(\"Total time of random forest classifier (s)\");\n",
    "plt.xlabel(\"Total time when always using the heuristics (s)\");\n",
    "plt.tight_layout();\n",
    "if name == \"leave\" or name == \"single\":\n",
    "    plt.xlim(left=0, right=max_time);\n",
    "    plt.ylim(bottom=0, top=max_time);\n",
    "else:\n",
    "    plt.xlim(left=0, right=4800);\n",
    "    plt.ylim(bottom=0, top=4800);\n",
    "#plt.savefig(\"class_heuristics_test.jpg\", bbox_inches = 'tight')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8));\n",
    "ax = plt.gca()\n",
    "if name == \"leave\" or name == \"single\":\n",
    "    max_time = int(max(df.query(\"train_test=='train'\")['tot_heur'].max(), df.query(\"train_test=='train'\")['tot_class'].max()))\n",
    "ax.scatter(df.query(\"train_test=='train'\")['tot_heur'], df.query(\"train_test=='train'\")['tot_class'], c=\"black\", marker=\"x\");\n",
    "ax.set_yscale('symlog')\n",
    "ax.set_xscale('symlog')\n",
    "if name == \"leave\" or name == \"single\":\n",
    "    plt.plot([0, max_time+5], [0, max_time+5], c=\"black\");\n",
    "else:\n",
    "    plt.plot([0, 4800+5], [0, 4800+5], c=\"black\");\n",
    "plt.title(\"Total time for problems in train set.\");\n",
    "plt.ylabel(\"Total time of random forest classifier (s).\");\n",
    "plt.xlabel(\"Total time when always using the heuristics (s).\");\n",
    "plt.tight_layout();\n",
    "if name == \"leave\" or name == \"single\":\n",
    "    plt.xlim(left=0, right=max_time);\n",
    "    plt.ylim(bottom=0, top=max_time);\n",
    "else:\n",
    "    plt.xlim(left=0, right=4800);\n",
    "    plt.ylim(bottom=0, top=4800);\n",
    "#plt.savefig(\"class_heuristics_train.jpg\", bbox_inches = 'tight')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_setting(tmp_solv, type_folder):\n",
    "    df_solvs = dict()\n",
    "    df_solvs_class = dict()\n",
    "    for tmp_set in [\"rf_all\", \"dt_subset\", \"rf_subset\", \"dt_all\"]:\n",
    "        tmp_dir_logs = \"../ML_logs/\"+tmp_set+\"/\"+tmp_solv+type_folder\n",
    "        tmp_files = os.listdir(tmp_dir_logs)\n",
    "        tmp_ground_truth = \"../Dataset/results_on_off_\"+tmp_solv+\".csv\"\n",
    "        df_solvs[tmp_set] = pd.read_csv(tmp_ground_truth)\n",
    "        df_solvs[tmp_set].drop_duplicates([\"Problem\", \"Policy\"], keep=\"first\", inplace=True)\n",
    "        targets = dict()\n",
    "        tot_time_without_tab = 0.0\n",
    "        tot_time_with_tab = 0.0\n",
    "        for prob in df_solvs[tmp_set][\"Problem\"].unique():\n",
    "            baseline = df_solvs[tmp_set].query(\"Problem=='\"+prob+\"' and Policy=='baseline'\")\n",
    "            if len(baseline) == 0:\n",
    "                continue\n",
    "            baseline_time = baseline['SolverTotalTime'].values[0] + baseline['SavileRowTotalTime'].values[0]\n",
    "            tab2 = result_df.query(\"Problem=='\"+prob+\"' and Policy=='2'\")\n",
    "            if len(tab2) == 0:\n",
    "                continue\n",
    "            tab2_time = tab2['SolverTotalTime'].values[0] + tab2['SavileRowTotalTime'].values[0]\n",
    "            baselineTimeOut = baseline[\"SolverTimeOut\"].fillna(1).values+baseline[\"SavileRowTimeOut\"].fillna(1).values\n",
    "            tab2TimeOut = tab2[\"SolverTimeOut\"].fillna(1).values+tab2[\"SavileRowTimeOut\"].fillna(1).values\n",
    "            if baselineTimeOut>0 and tab2TimeOut>0:\n",
    "                continue  # drop row\n",
    "            if baselineTimeOut>0 and tab2TimeOut==0:\n",
    "                clipped = np.clip(tab2_time, 0, 3600)\n",
    "                targets[prob] = {'Target': 1, 'Score': 3600-clipped, 'Tab_t': clipped, 'Base_t': 3600}\n",
    "            elif baselineTimeOut==0 and tab2TimeOut>0:\n",
    "                clipped = np.clip(baseline_time, 0, 3600)\n",
    "                targets[prob] = {'Target': 0, 'Score': clipped-3600, 'Tab_t': 3600, 'Base_t': clipped}\n",
    "            elif baselineTimeOut==0 and tab2TimeOut==0 and np.abs(tab2_time-baseline_time)<1:\n",
    "                continue # Skip small differences\n",
    "            elif baselineTimeOut==0 and tab2TimeOut==0:# and np.abs(tab2_time-baseline_time)>=1:\n",
    "                label = 1 if tab2_time < baseline_time else 0\n",
    "                clipped_1 = np.clip(tab2_time, 0, 3600)\n",
    "                clipped_2 = np.clip(baseline_time, 0, 3600)\n",
    "                targets[prob] = {'Target': label, 'Score': clipped_2-clipped_1, 'Tab_t': clipped_1, 'Base_t': clipped_2}\n",
    "        target_df = pd.DataFrame.from_dict(targets, orient ='index')\n",
    "        target_df['Problem'] = target_df.index\n",
    "        target_df['similar'] = target_df.apply(lambda row: 1 if (row['Tab_t']<=60 and row['Base_t']<=60) or np.abs(row['Tab_t'] - row['Base_t']) < 0.1*min([row['Base_t'], row['Tab_t']]) else 0, axis=1)\n",
    "        df_solvs[tmp_set] = target_df.copy()\n",
    "        name = tmp_dir_logs.split(\"/\")[-1].split(\"_\")[0]\n",
    "        df = pd.DataFrame()\n",
    "        if (name == \"instance\" or name == \"problem\"):\n",
    "            problems = []\n",
    "            for file in tmp_files:\n",
    "                filename = os.path.join(tmp_dir_logs, file)\n",
    "                with open(filename, \"rb\") as pkl_f:\n",
    "                    test_problems, train_problems, all_test, all_train = pickle.load(pkl_f)\n",
    "                problems.append([test_problems, train_problems])\n",
    "                df = pd.concat([df, pd.DataFrame(all_test)])\n",
    "                df = pd.concat([df, pd.DataFrame(all_train)])\n",
    "            df = df.reset_index().drop([\"index\"], axis=1)\n",
    "        elif name == \"single\":\n",
    "            for file in tmp_files:\n",
    "                complete_df = pd.DataFrame()\n",
    "                filename = os.path.join(tmp_dir_logs, file)\n",
    "                with open(filename, \"rb\") as pkl_f:\n",
    "                    all_test, all_train = pickle.load(pkl_f)\n",
    "                df = pd.concat([df, pd.DataFrame(all_test)])\n",
    "                df = pd.concat([df, pd.DataFrame(all_train)])\n",
    "                df.reset_index().drop([\"index\"], axis=1)\n",
    "        elif name == \"leave\":\n",
    "            complete_df = pd.DataFrame()\n",
    "            for file in tmp_files:\n",
    "                filename = os.path.join(tmp_dir_logs, file)\n",
    "                with open(filename, \"rb\") as pkl_f:\n",
    "                    all_test = pickle.load(pkl_f)[0]\n",
    "                complete_df = pd.concat([complete_df, pd.DataFrame(all_test)])\n",
    "                #complete_df = pd.concat([complete_df, pd.DataFrame(all_train)])\n",
    "            df = complete_df.reset_index().drop([\"index\"], axis=1)\n",
    "        \n",
    "        if name == \"instance\" or name == \"problem\":\n",
    "            complete_df = df.copy()\n",
    "            df_new = pd.DataFrame()\n",
    "            num_cols = [x for x in complete_df.columns if x not in [\"Problem\", \"train_test\", \"prediction\"]]\n",
    "            for prob in complete_df['Problem'].unique():\n",
    "                prob_df = complete_df.query(\"Problem=='\"+prob+\"'\")\n",
    "                for train_test in prob_df['train_test'].unique():\n",
    "                    a = prob_df.query(\"train_test=='\"+train_test+\"'\")\n",
    "                    tmp_df = a[num_cols].mean().to_frame().T\n",
    "                    tmp_df[\"Problem\"] = prob\n",
    "                    tmp_df[\"train_test\"] = train_test\n",
    "                    tmp_df[\"prediction\"] = 1 if (a[\"prediction\"]==a[\"Problem\"]).sum()>=len(a[\"Problem\"])/2 else 0\n",
    "                    df_new = pd.concat([df_new, tmp_df])\n",
    "            df = df_new.reset_index().drop([\"index\"], axis=1)\n",
    "            df[\"saved_class_v_heur\"] = df[\"saved_class\"]-df[\"saved_heur\"]\n",
    "        elif name == \"single\" or name==\"leave\":\n",
    "            if name == \"single\":\n",
    "                df[\"saved_class\"] /= df.apply(lambda x: len(x[\"prediction\"]), axis=1)\n",
    "                df[\"saved_v_best\"] /= df.apply(lambda x: len(x[\"prediction\"]), axis=1)\n",
    "                df[\"saved_heur\"] /= df.apply(lambda x: len(x[\"prediction\"]), axis=1)\n",
    "                df[\"tot_best\"] /= df.apply(lambda x: len(x[\"prediction\"]), axis=1)\n",
    "                df[\"tot_class\"] /= df.apply(lambda x: len(x[\"prediction\"]), axis=1)\n",
    "                df[\"tot_heur\"] /= df.apply(lambda x: len(x[\"prediction\"]), axis=1)\n",
    "                df[\"tot_no_tab\"] /= df.apply(lambda x: len(x[\"prediction\"]), axis=1)\n",
    "            else:\n",
    "                df[\"saved_class\"] /= df[\"num_instances\"]\n",
    "                df[\"saved_v_best\"] /= df[\"num_instances\"]\n",
    "                df[\"saved_heur\"] /= df[\"num_instances\"]\n",
    "                df[\"tot_best\"] /= df[\"num_instances\"]\n",
    "                df[\"tot_class\"] /= df[\"num_instances\"]\n",
    "                df[\"tot_heur\"] /= df[\"num_instances\"]\n",
    "                df[\"tot_no_tab\"] /= df[\"num_instances\"]\n",
    "            df = df.groupby([\"Problem\", \"train_test\"]).agg({\n",
    "                \"saved_v_best\": 'mean',\n",
    "                \"saved_class\": 'mean',\n",
    "                \"saved_heur\": 'mean',\n",
    "                \"tot_best\": 'mean',\n",
    "                \"tot_class\": 'mean',\n",
    "                \"tot_heur\": 'mean',\n",
    "                \"tot_no_tab\": 'mean',\n",
    "                #\"prediction\": 'first'\n",
    "            })\n",
    "            df = df.reset_index()\n",
    "            df[\"saved_class_v_heur\"] = df[\"saved_class\"]-df[\"saved_heur\"]\n",
    "        df_solvs_class[tmp_set] = df.copy()\n",
    "    return df_solvs, df_solvs_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plots of different settings and subsets of features for a given solver\n",
    "tmp_solv = \"minion\"\n",
    "dfs_boxs = pd.DataFrame()\n",
    "for setting in [\"instance\", \"single_problem\", \"leave_one_out\"]:\n",
    "    type_folder = \"/\"+setting+\"_custom\"\n",
    "    _, df_solvs_class = run_setting(tmp_solv, type_folder)\n",
    "    \n",
    "    df_list = []\n",
    "    for subset_t in [\"rf_all\", \"dt_subset\", \"rf_subset\", \"dt_all\"]:\n",
    "        df_solvs_class[subset_t][\"Problem\"] = df_solvs_class[subset_t].apply(lambda x: x[\"Problem\"].split(\"_\")[0], axis=1)\n",
    "        df_solvs_class[subset_t] = df_solvs_class[subset_t].groupby([\"Problem\", \"train_test\"]).agg(\"mean\", numeric_only=True).reset_index()\n",
    "        \n",
    "        if setting == \"single_problem\":\n",
    "                df_solvs_class[subset_t][\"saved_s_best\"] = 0\n",
    "                for prob in df_solvs_class[subset_t].Problem.unique():\n",
    "                    a = df_solvs_class[subset_t].query(\"train_test=='train' and Problem=='\"+prob+\"'\")\n",
    "                    mask = df_solvs_class[subset_t][\"Problem\"].str.match(prob) & df_solvs_class[subset_t][\"train_test\"].str.match(\"train\")\n",
    "                    df_solvs_class[subset_t].loc[mask, \"saved_s_best\"] = 0 if a[\"saved_heur\"].iloc[0]<=0 else a[\"saved_heur\"]\n",
    "                    b = df_solvs_class[subset_t].query(\"train_test=='test' and Problem=='\"+prob+\"'\")\n",
    "                    mask = df_solvs_class[subset_t][\"Problem\"].str.match(prob) & df_solvs_class[subset_t][\"train_test\"].str.match(\"test\")\n",
    "                    df_solvs_class[subset_t].loc[mask, \"saved_s_best\"] = 0 if a[\"saved_heur\"].iloc[0]<=0 else b[\"saved_heur\"]\n",
    "                    df_solvs_class[subset_t].loc[mask, \"tot_s_best\"] = df_solvs_class[subset_t].loc[mask, \"tot_no_tab\"] - df_solvs_class[subset_t].loc[mask, \"saved_s_best\"]\n",
    "                    df_solvs_class[subset_t].loc[mask, \"saved_s_best\"] = b[\"saved_class\"] - df_solvs_class[subset_t].loc[mask, \"saved_s_best\"]\n",
    "        elif setting == \"leave_one_out\":\n",
    "            df_solvs_class[subset_t][\"saved_s_best\"] = 0\n",
    "            for prob in df_solvs_class[subset_t].Problem.unique():\n",
    "                a = df_solvs_class[subset_t].query(\"train_test=='test' and Problem!='\"+prob+\"'\")\n",
    "                b = df_solvs_class[subset_t].query(\"train_test=='test' and Problem=='\"+prob+\"'\")\n",
    "                mask = df_solvs_class[subset_t][\"Problem\"].str.match(prob) & df_solvs_class[subset_t][\"train_test\"].str.match(\"test\")\n",
    "                df_solvs_class[subset_t].loc[mask, \"saved_s_best\"] = 0 if a[\"saved_heur\"].sum()<=0 else b[\"saved_heur\"]\n",
    "                df_solvs_class[subset_t].loc[mask, \"tot_s_best\"] = df_solvs_class[subset_t].loc[mask, \"tot_no_tab\"] - df_solvs_class[subset_t].loc[mask, \"saved_s_best\"]\n",
    "                df_solvs_class[subset_t].loc[mask, \"saved_s_best\"] = b[\"saved_class\"] - df_solvs_class[subset_t].loc[mask, \"saved_s_best\"]\n",
    "        elif setting == \"instance\":\n",
    "            saved_heur = df_solvs_class[subset_t][df_solvs_class[subset_t][\"train_test\"]=='train'].sample(frac=0.7).sum()[\"saved_heur\"]\n",
    "            df_solvs_class[subset_t] = df_solvs_class[subset_t][df_solvs_class[subset_t][\"train_test\"]=='test']\n",
    "            df_solvs_class[subset_t] = df_solvs_class[subset_t].reset_index()\n",
    "            df_solvs_class[subset_t][\"Problem\"] = df_solvs_class[subset_t][\"index\"]\n",
    "            df_solvs_class[subset_t] = df_solvs_class[subset_t].drop([\"index\", \"prediction\"], axis=1)\n",
    "            df_solvs_class[subset_t][\"saved_s_best\"] = 0 if saved_heur<=0 else df_solvs_class[subset_t][\"saved_heur\"]\n",
    "            df_solvs_class[subset_t][\"tot_s_best\"] = df_solvs_class[subset_t][\"tot_no_tab\"] - df_solvs_class[subset_t][\"saved_s_best\"]\n",
    "            df_solvs_class[subset_t][\"saved_s_best\"] = df_solvs_class[subset_t][\"saved_class\"] - df_solvs_class[subset_t][\"saved_s_best\"]\n",
    "        \n",
    "        if \"prediction\" in df_solvs_class[subset_t].columns:\n",
    "            df_list.append(df_solvs_class[subset_t].query(\"train_test=='test'\").drop([\"prediction\", \"train_test\"], axis=1).copy().assign(Trial=subset_t))\n",
    "        elif \"train_test\" in df_solvs_class[subset_t].columns:\n",
    "            df_list.append(df_solvs_class[subset_t].query(\"train_test=='test'\").drop([\"train_test\"], axis=1).copy().assign(Trial=subset_t))\n",
    "        else:\n",
    "            df_list.append(df_solvs_class[subset_t].copy().assign(Trial=subset_t))\n",
    "\n",
    "        if setting == \"instance\":\n",
    "            df_list[-1] = df_list[-1].copy().assign(Trial=subset_t)\n",
    "            cols = [x for x in df_list[-1].columns if x != \"Trial\" and x!= \"Problem\"]\n",
    "            df_list[-1][cols] = df_list[-1][cols].astype(float)\n",
    "            \n",
    "        df_list[-1][\"tot_class_log\"] = np.log(df_list[-1][\"tot_class\"])\n",
    "    \n",
    "        df_list[-1][\"tot_class - \"+subset_t] = df_list[-1][\"tot_class\"]\n",
    "        df_list[-1][\"tot_best - \"+subset_t] = df_list[-1][\"tot_best\"]\n",
    "        df_list[-1][\"tot_s_best - \"+subset_t] = df_list[-1][\"tot_s_best\"]\n",
    "        df_list[-1][\"tot_class_log - \"+subset_t] = df_list[-1][\"tot_class_log\"]\n",
    "        df_list[-1][\"saved_class - \"+subset_t] = df_list[-1][\"saved_class\"]\n",
    "        df_list[-1][\"saved_class_v_heur - \"+subset_t] = df_list[-1][\"saved_class_v_heur\"]\n",
    "        df_list[-1][\"saved_s_best - \"+subset_t] = df_list[-1][\"saved_s_best\"]\n",
    "    \n",
    "        df_list[-1] = df_list[-1][[\"tot_best - \"+subset_t, \"tot_s_best - \"+subset_t, \"tot_class - \"+subset_t, \"tot_class_log - \"+subset_t, \"saved_class - \"+subset_t, \"saved_class_v_heur - \"+subset_t, \"saved_s_best - \"+subset_t, \"Problem\"]]\n",
    "    \n",
    "    dfs_boxs_tmp = df_list[0].merge(df_list[-1], on=\"Problem\")\n",
    "    dfs_boxs_tmp = dfs_boxs_tmp.merge(df_list[1], on=\"Problem\")\n",
    "    dfs_boxs_tmp = dfs_boxs_tmp.merge(df_list[2], on=\"Problem\")\n",
    "    #dfs_boxs_tmp = dfs_boxs_tmp.merge(df_list[3], on=\"Problem\")\n",
    "    #dfs_boxs_tmp = dfs_boxs_tmp.merge(df_list[4], on=\"Problem\")\n",
    "    #dfs_boxs_tmp = dfs_boxs_tmp.merge(df_list[5], on=\"Problem\")\n",
    "    #dfs_boxs_tmp = dfs_boxs_tmp.merge(df_list[6], on=\"Problem\")\n",
    "    dfs_boxs_tmp = dfs_boxs_tmp.drop([\"Problem\"], axis=1)\n",
    "    \n",
    "    dfs_boxs_tmp[\"Setting\"] = \" \".join([x[0].upper()+x[1:] for x in setting.split(\"_\")])\n",
    "    dfs_boxs_tmp[\"Setting\"] = dfs_boxs_tmp[\"Setting\"].apply(lambda x: \"By Instance\" if x==\"Instance\" else x)\n",
    "    dfs_boxs_tmp[\"Setting\"] = dfs_boxs_tmp[\"Setting\"].apply(lambda x: \"Per Problem Class\" if x==\"Single Problem\" else x)\n",
    "    \n",
    "    dfs_boxs = pd.concat([dfs_boxs, dfs_boxs_tmp])\n",
    "    \n",
    "    print(\"Finished \" + setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "plt.rc('font', size=12) #controls default text size\n",
    "plt.rc('legend', fontsize=10) #fontsize of the legend\n",
    "meanprops = {\"marker\":\"^\",\"markerfacecolor\":\"green\", \"markeredgecolor\":\"lightgreen\"}\n",
    "my_pal = {\"RF subset\": \"red\", \"RF all\": \"darkred\", \"DT all\":\"darkblue\", \"DT subset\":\"blue\"}\n",
    "\n",
    "a = dfs_boxs[[\"Setting\"]+[\"tot_class - \"+subset_t for subset_t in [\"rf_all\", \"dt_subset\", \"rf_subset\", \"dt_all\"]]].copy()\n",
    "a[\"tot_class - RF subset\"] = a[\"tot_class - rf_subset\"]\n",
    "a[\"tot_class - DT subset\"] = a[\"tot_class - dt_subset\"]\n",
    "a[\"tot_class - DT all\"] = a[\"tot_class - dt_all\"]\n",
    "a[\"tot_class - RF all\"] = a[\"tot_class - rf_all\"]\n",
    "a = a.drop([x for x in a.columns if \"RF\" not in x and \"DT\" not in x and x!=\"Setting\"], axis=1)\n",
    "for c in a.columns:\n",
    "    if c != \"Setting\":\n",
    "        a[c.split(\" - \")[1]] = a[c]\n",
    "        a = a.drop([c], axis=1)\n",
    "a = a.reset_index()\n",
    "a = a.drop([\"index\"], axis=1)\n",
    "\n",
    "mdf = pd.melt(a, id_vars=['Setting'], var_name=['Method'])\n",
    "mask = (mdf[\"value\"]>threshold) | (mdf[\"value\"]<-threshold)\n",
    "mdf = mdf[mask]\n",
    "\n",
    "if len(mdf) > 1:\n",
    "    sns.boxplot(x='Setting', y=\"value\", hue=\"Method\", data=mdf, \n",
    "                hue_order=sorted(mdf[\"Method\"].unique()), showmeans=True, showfliers = False, \n",
    "                meanprops=meanprops, palette=my_pal);\n",
    "\n",
    "    plt.title(tmp_solv[0].upper()+\" \".join(tmp_solv[1:].split(\"_\")));\n",
    "    plt.ylabel(\"Total time (s)\");\n",
    "    plt.tight_layout();\n",
    "    #plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.axhline(0, color='black', linestyle='dotted')\n",
    "    #plt.savefig(\"total_feat_settings_boxplot_\"+tmp_solv+\".jpg\", bbox_inches = 'tight');\n",
    "    plt.show()\n",
    "\n",
    "a = dfs_boxs[[\"Setting\"]+[\"tot_class_log - \"+subset_t for subset_t in [\"rf_all\", \"dt_subset\", \"rf_subset\", \"dt_all\"]]].copy()\n",
    "a[\"tot_class_log - RF subset\"] = a[\"tot_class_log - rf_subset\"]\n",
    "a[\"tot_class_log - DT subset\"] = a[\"tot_class_log - dt_subset\"]\n",
    "a[\"tot_class_log - DT all\"] = a[\"tot_class_log - dt_all\"]\n",
    "a[\"tot_class_log - RF all\"] = a[\"tot_class_log - rf_all\"]\n",
    "a = a.drop([x for x in a.columns if \"RF\" not in x and \"DT\" not in x and x!=\"Setting\"], axis=1)\n",
    "for c in a.columns:\n",
    "    if c != \"Setting\":\n",
    "        a[c.split(\" - \")[1]] = a[c]\n",
    "        a = a.drop([c], axis=1)\n",
    "a = a.reset_index()\n",
    "a = a.drop([\"index\"], axis=1)\n",
    "\n",
    "mdf = pd.melt(a, id_vars=['Setting'], var_name=['Method'])\n",
    "mask = (mdf[\"value\"]>threshold) | (mdf[\"value\"]<-threshold)\n",
    "mdf = mdf[mask]\n",
    "\n",
    "if len(mdf) > 1:\n",
    "    sns.boxplot(x='Setting', y=\"value\", hue=\"Method\", data=mdf, \n",
    "                hue_order=sorted(mdf[\"Method\"].unique()), showmeans=True, \n",
    "                meanprops=meanprops, palette=my_pal);\n",
    "\n",
    "    plt.title(tmp_solv[0].upper()+\" \".join(tmp_solv[1:].split(\"_\")));\n",
    "    plt.ylabel(\"Total time in log scale (s)\");\n",
    "    plt.tight_layout();\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.axhline(0, color='black', linestyle='dotted')\n",
    "    #plt.savefig(\"total_feat_settings_boxplot_log_\"+tmp_solv+\".jpg\", bbox_inches = 'tight');\n",
    "    plt.show()\n",
    "\n",
    "a = dfs_boxs[[\"Setting\"]+[\"saved_class - \"+subset_t for subset_t in [\"rf_all\", \"dt_subset\", \"rf_subset\", \"dt_all\"]]].copy()\n",
    "a[\"saved_class - RF subset\"] = a[\"saved_class - rf_subset\"]\n",
    "a[\"saved_class - DT subset\"] = a[\"saved_class - dt_subset\"]\n",
    "a[\"saved_class - DT all\"] = a[\"saved_class - dt_all\"]\n",
    "a[\"saved_class - RF all\"] = a[\"saved_class - rf_all\"]\n",
    "a = a.drop([x for x in a.columns if \"RF\" not in x and \"DT\" not in x and x!=\"Setting\"], axis=1)\n",
    "for c in a.columns:\n",
    "    if c != \"Setting\":\n",
    "        a[c.split(\" - \")[1]] = a[c]\n",
    "        a = a.drop([c], axis=1)\n",
    "a = a.reset_index()\n",
    "a = a.drop([\"index\"], axis=1)\n",
    "\n",
    "mdf = pd.melt(a, id_vars=['Setting'], var_name=['Method'])\n",
    "mask = (mdf[\"value\"]>threshold) | (mdf[\"value\"]<-threshold)\n",
    "mdf = mdf[mask]\n",
    "\n",
    "if len(mdf) > 1:\n",
    "    sns.boxplot(x='Setting', y=\"value\", hue=\"Method\", data=mdf, \n",
    "                hue_order=sorted(mdf[\"Method\"].unique()), showmeans=True, \n",
    "                showfliers = False, meanprops=meanprops, palette=my_pal);\n",
    "\n",
    "    plt.title(tmp_solv[0].upper()+\" \".join(tmp_solv[1:].split(\"_\")));\n",
    "    plt.ylabel(\"Saved time with respect to never using tabulation (s)\");\n",
    "    plt.tight_layout();\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.axhline(0, color='black', linestyle='dotted')\n",
    "    #plt.savefig(\"saved_feat_settings_boxplot_\"+tmp_solv+\".jpg\", bbox_inches = 'tight');\n",
    "    plt.show()\n",
    "    \n",
    "a = dfs_boxs[[\"Setting\"]+[\"saved_class_v_heur - \"+subset_t for subset_t in [\"rf_all\", \"dt_subset\", \"rf_subset\", \"dt_all\"]]].copy()\n",
    "a[\"saved_class_v_heur - RF subset\"] = a[\"saved_class_v_heur - rf_subset\"]\n",
    "a[\"saved_class_v_heur - DT subset\"] = a[\"saved_class_v_heur - dt_subset\"]\n",
    "a[\"saved_class_v_heur - DT all\"] = a[\"saved_class_v_heur - dt_all\"]\n",
    "a[\"saved_class_v_heur - RF all\"] = a[\"saved_class_v_heur - rf_all\"]\n",
    "a = a.drop([x for x in a.columns if \"RF\" not in x and \"DT\" not in x and x!=\"Setting\"], axis=1)\n",
    "for c in a.columns:\n",
    "    if c != \"Setting\":\n",
    "        a[c.split(\" - \")[1]] = a[c]\n",
    "        a = a.drop([c], axis=1)\n",
    "a = a.reset_index()\n",
    "a = a.drop([\"index\"], axis=1)\n",
    "\n",
    "mdf = pd.melt(a, id_vars=['Setting'], var_name=['Method'])\n",
    "mask = (mdf[\"value\"]>threshold) | (mdf[\"value\"]<-threshold)\n",
    "mdf = mdf[mask]\n",
    "\n",
    "if len(mdf) > 1:\n",
    "    sns.boxplot(x='Setting', y=\"value\", hue=\"Method\", data=mdf, \n",
    "                hue_order=sorted(mdf[\"Method\"].unique()), showmeans=True, \n",
    "                showfliers = False, meanprops=meanprops, palette=my_pal);\n",
    "\n",
    "    plt.title(tmp_solv[0].upper()+\" \".join(tmp_solv[1:].split(\"_\")));\n",
    "    plt.ylabel(\"Saved time with respect to the heuristics (s)\");\n",
    "    plt.tight_layout();\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.axhline(0, color='black', linestyle='dotted')\n",
    "    #plt.savefig(\"saved_v_heur_feat_settings_boxplot_\"+tmp_solv+\".jpg\", bbox_inches = 'tight');\n",
    "    plt.show()\n",
    "             \n",
    "a = dfs_boxs[[\"Setting\"]+[\"saved_s_best - \"+subset_t for subset_t in [\"rf_all\", \"dt_subset\", \"rf_subset\", \"dt_all\"]]].copy()\n",
    "a[\"saved_s_best - RF subset\"] = a[\"saved_s_best - rf_subset\"]\n",
    "a[\"saved_s_best - DT subset\"] = a[\"saved_s_best - dt_subset\"]\n",
    "a[\"saved_s_best - DT all\"] = a[\"saved_s_best - dt_all\"]\n",
    "a[\"saved_s_best - RF all\"] = a[\"saved_s_best - rf_all\"]\n",
    "a = a.drop([x for x in a.columns if \"RF\" not in x and \"DT\" not in x and x!=\"Setting\"], axis=1)\n",
    "for c in a.columns:\n",
    "    if c != \"Setting\":\n",
    "        a[c.split(\" - \")[1]] = a[c]\n",
    "        a = a.drop([c], axis=1)\n",
    "a = a.reset_index()\n",
    "a = a.drop([\"index\"], axis=1)\n",
    "\n",
    "mdf = pd.melt(a, id_vars=['Setting'], var_name=['Method'])\n",
    "mask = (mdf[\"value\"]>threshold) | (mdf[\"value\"]<-threshold)\n",
    "mdf = mdf[mask]\n",
    "\n",
    "if len(mdf) > 1:\n",
    "    sns.boxplot(x='Setting', y=\"value\", hue=\"Method\", data=mdf, \n",
    "                hue_order=sorted(mdf[\"Method\"].unique()), showmeans=True, \n",
    "                showfliers = False, meanprops=meanprops, palette=my_pal);\n",
    "\n",
    "    plt.title(tmp_solv[0].upper()+\" \".join(tmp_solv[1:].split(\"_\")));\n",
    "    plt.ylabel(\"Saved time (s)\");\n",
    "    plt.tight_layout();\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.axhline(0, color='black', linestyle='dotted')\n",
    "    #plt.savefig(\"saved_s_best_feat_settings_boxplot_\"+tmp_solv+\".jpg\", bbox_inches = 'tight');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_setting(type_folder):\n",
    "    df_solvs = dict()\n",
    "    df_solvs_class = dict()\n",
    "    for tmp_solv in [\"minion\", \"kissat\", \"kissat_mdd\", \"chuffed\"]:\n",
    "        tmp_dir_logs = \"../ML_logs/rf_all/\"+tmp_solv+type_folder\n",
    "        tmp_files = os.listdir(tmp_dir_logs)\n",
    "        tmp_ground_truth = \"../Dataset/results_on_off_\"+tmp_solv+\".csv\"\n",
    "        df_solvs[tmp_solv] = pd.read_csv(tmp_ground_truth)\n",
    "        df_solvs[tmp_solv].drop_duplicates([\"Problem\", \"Policy\"], keep=\"first\", inplace=True)\n",
    "        targets = dict()\n",
    "        tot_time_without_tab = 0.0\n",
    "        tot_time_with_tab = 0.0\n",
    "        for prob in df_solvs[tmp_solv][\"Problem\"].unique():\n",
    "            baseline = df_solvs[tmp_solv].query(\"Problem=='\"+prob+\"' and Policy=='baseline'\")\n",
    "            if len(baseline) == 0:\n",
    "                continue\n",
    "            baseline_time = baseline['SolverTotalTime'].values[0] + baseline['SavileRowTotalTime'].values[0]\n",
    "            tab2 = result_df.query(\"Problem=='\"+prob+\"' and Policy=='2'\")\n",
    "            if len(tab2) == 0:\n",
    "                continue\n",
    "            tab2_time = tab2['SolverTotalTime'].values[0] + tab2['SavileRowTotalTime'].values[0]\n",
    "            baselineTimeOut = baseline[\"SolverTimeOut\"].fillna(1).values+baseline[\"SavileRowTimeOut\"].fillna(1).values\n",
    "            tab2TimeOut = tab2[\"SolverTimeOut\"].fillna(1).values+tab2[\"SavileRowTimeOut\"].fillna(1).values\n",
    "            if baselineTimeOut>0 and tab2TimeOut>0:\n",
    "                continue  # drop row\n",
    "            if baselineTimeOut>0 and tab2TimeOut==0:\n",
    "                clipped = np.clip(tab2_time, 0, 3600)\n",
    "                targets[prob] = {'Target': 1, 'Score': 3600-clipped, 'Tab_t': clipped, 'Base_t': 3600}\n",
    "            elif baselineTimeOut==0 and tab2TimeOut>0:\n",
    "                clipped = np.clip(baseline_time, 0, 3600)\n",
    "                targets[prob] = {'Target': 0, 'Score': clipped-3600, 'Tab_t': 3600, 'Base_t': clipped}\n",
    "            elif baselineTimeOut==0 and tab2TimeOut==0 and np.abs(tab2_time-baseline_time)<1:\n",
    "                continue # Skip small differences\n",
    "            elif baselineTimeOut==0 and tab2TimeOut==0:# and np.abs(tab2_time-baseline_time)>=1:\n",
    "                label = 1 if tab2_time < baseline_time else 0\n",
    "                clipped_1 = np.clip(tab2_time, 0, 3600)\n",
    "                clipped_2 = np.clip(baseline_time, 0, 3600)\n",
    "                targets[prob] = {'Target': label, 'Score': clipped_2-clipped_1, 'Tab_t': clipped_1, 'Base_t': clipped_2}\n",
    "        target_df = pd.DataFrame.from_dict(targets, orient ='index')\n",
    "        target_df['Problem'] = target_df.index\n",
    "        target_df['similar'] = target_df.apply(lambda row: 1 if (row['Tab_t']<=60 and row['Base_t']<=60) or np.abs(row['Tab_t'] - row['Base_t']) < 0.1*min([row['Base_t'], row['Tab_t']]) else 0, axis=1)\n",
    "        df_solvs[tmp_solv] = target_df.copy()\n",
    "        name = tmp_dir_logs.split(\"/\")[-1].split(\"_\")[0]\n",
    "        df = pd.DataFrame()\n",
    "        if (name == \"instance\" or name == \"problem\"):\n",
    "            problems = []\n",
    "            for file in tmp_files:\n",
    "                filename = os.path.join(tmp_dir_logs, file)\n",
    "                with open(filename, \"rb\") as pkl_f:\n",
    "                    test_problems, train_problems, all_test, all_train = pickle.load(pkl_f)\n",
    "                problems.append([test_problems, train_problems])\n",
    "                df = pd.concat([df, pd.DataFrame(all_test)])\n",
    "                df = pd.concat([df, pd.DataFrame(all_train)])\n",
    "            df = df.reset_index().drop([\"index\"], axis=1)\n",
    "        elif name == \"single\":\n",
    "            for file in tmp_files:\n",
    "                complete_df = pd.DataFrame()\n",
    "                filename = os.path.join(tmp_dir_logs, file)\n",
    "                with open(filename, \"rb\") as pkl_f:\n",
    "                    all_test, all_train = pickle.load(pkl_f)\n",
    "                df = pd.concat([df, pd.DataFrame(all_test)])\n",
    "                df = pd.concat([df, pd.DataFrame(all_train)])\n",
    "                df.reset_index().drop([\"index\"], axis=1)\n",
    "        elif name == \"leave\":\n",
    "            complete_df = pd.DataFrame()\n",
    "            for file in tmp_files:\n",
    "                filename = os.path.join(tmp_dir_logs, file)\n",
    "                with open(filename, \"rb\") as pkl_f:\n",
    "                    all_test = pickle.load(pkl_f)[0]\n",
    "                complete_df = pd.concat([complete_df, pd.DataFrame(all_test)])\n",
    "                #complete_df = pd.concat([complete_df, pd.DataFrame(all_train)])\n",
    "            df = complete_df.reset_index().drop([\"index\"], axis=1)\n",
    "        \n",
    "        if name == \"instance\" or name == \"problem\":\n",
    "            complete_df = df.copy()\n",
    "            df_new = pd.DataFrame()\n",
    "            num_cols = [x for x in complete_df.columns if x not in [\"Problem\", \"train_test\", \"prediction\"]]\n",
    "            for prob in complete_df['Problem'].unique():\n",
    "                prob_df = complete_df.query(\"Problem=='\"+prob+\"'\")\n",
    "                for train_test in prob_df['train_test'].unique():\n",
    "                    a = prob_df.query(\"train_test=='\"+train_test+\"'\")\n",
    "                    tmp_df = a[num_cols].mean().to_frame().T\n",
    "                    tmp_df[\"Problem\"] = prob\n",
    "                    tmp_df[\"train_test\"] = train_test\n",
    "                    tmp_df[\"prediction\"] = 1 if (a[\"prediction\"]==a[\"Problem\"]).sum()>=len(a[\"Problem\"])/2 else 0\n",
    "                    df_new = pd.concat([df_new, tmp_df])\n",
    "            df = df_new.reset_index().drop([\"index\"], axis=1)\n",
    "            df[\"saved_class_v_heur\"] = df[\"saved_class\"]-df[\"saved_heur\"]\n",
    "        elif name == \"single\" or name==\"leave\":\n",
    "            if name == \"single\":\n",
    "                df[\"saved_class\"] /= df.apply(lambda x: len(x[\"prediction\"]), axis=1)\n",
    "                df[\"saved_v_best\"] /= df.apply(lambda x: len(x[\"prediction\"]), axis=1)\n",
    "                df[\"saved_heur\"] /= df.apply(lambda x: len(x[\"prediction\"]), axis=1)\n",
    "                df[\"tot_best\"] /= df.apply(lambda x: len(x[\"prediction\"]), axis=1)\n",
    "                df[\"tot_class\"] /= df.apply(lambda x: len(x[\"prediction\"]), axis=1)\n",
    "                df[\"tot_heur\"] /= df.apply(lambda x: len(x[\"prediction\"]), axis=1)\n",
    "                df[\"tot_no_tab\"] /= df.apply(lambda x: len(x[\"prediction\"]), axis=1)\n",
    "            else:\n",
    "                df[\"saved_class\"] /= df[\"num_instances\"]\n",
    "                df[\"saved_v_best\"] /= df[\"num_instances\"]\n",
    "                df[\"saved_heur\"] /= df[\"num_instances\"]\n",
    "                df[\"tot_best\"] /= df[\"num_instances\"]\n",
    "                df[\"tot_class\"] /= df[\"num_instances\"]\n",
    "                df[\"tot_heur\"] /= df[\"num_instances\"]\n",
    "                df[\"tot_no_tab\"] /= df[\"num_instances\"]\n",
    "            df = df.groupby([\"Problem\", \"train_test\"]).agg({\n",
    "                \"saved_v_best\": 'mean',\n",
    "                \"saved_class\": 'mean',\n",
    "                \"saved_heur\": 'mean',\n",
    "                \"tot_best\": 'mean',\n",
    "                \"tot_class\": 'mean',\n",
    "                \"tot_heur\": 'mean',\n",
    "                \"tot_no_tab\": 'mean',\n",
    "                #\"prediction\": 'first'\n",
    "            })\n",
    "            df = df.reset_index()\n",
    "            df[\"saved_class_v_heur\"] = df[\"saved_class\"]-df[\"saved_heur\"]\n",
    "        df_solvs_class[tmp_solv] = df.copy()\n",
    "    return df_solvs, df_solvs_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots of different settings and solvers, against a given setting (no tabulation, heuristics, single best)\n",
    "dfs_boxs = pd.DataFrame()\n",
    "for setting in [\"leave_one_out\", \"single_problem\", \"instance\"]:\n",
    "    type_folder = \"/\"+setting+\"_custom\"\n",
    "    _, df_solvs_class = run_setting(type_folder)\n",
    "    \n",
    "    df_list = []\n",
    "    for tmp_solv in [\"minion\", \"kissat\", \"kissat_mdd\", \"chuffed\"]:\n",
    "        \n",
    "        if setting == \"single_problem\":\n",
    "            df_solvs_class[tmp_solv][\"saved_s_best\"] = 0\n",
    "            for prob in df_solvs_class[tmp_solv].Problem.unique():\n",
    "                a = df_solvs_class[tmp_solv].query(\"train_test=='train' and Problem=='\"+prob+\"'\")\n",
    "                b = df_solvs_class[tmp_solv].query(\"train_test=='test' and Problem=='\"+prob+\"'\")\n",
    "                mask = df_solvs_class[tmp_solv][\"Problem\"].str.match(prob) & df_solvs_class[tmp_solv][\"train_test\"].str.match(\"test\")\n",
    "                df_solvs_class[tmp_solv].loc[mask, \"saved_s_best\"] = 0 if a[\"saved_heur\"].sum()<=0 else b[\"saved_heur\"]\n",
    "                df_solvs_class[tmp_solv].loc[mask, \"saved_s_best\"] = b[\"saved_class\"] - df_solvs_class[tmp_solv].loc[mask, \"saved_s_best\"]\n",
    "        elif setting == \"leave_one_out\":\n",
    "            df_solvs_class[tmp_solv][\"saved_s_best\"] = 0\n",
    "            for prob in df_solvs_class[tmp_solv].Problem.unique():\n",
    "                a = df_solvs_class[tmp_solv].query(\"train_test=='test' and Problem!='\"+prob+\"'\")\n",
    "                b = df_solvs_class[tmp_solv].query(\"train_test=='test' and Problem=='\"+prob+\"'\")\n",
    "                mask = df_solvs_class[tmp_solv][\"Problem\"].str.match(prob) & df_solvs_class[tmp_solv][\"train_test\"].str.match(\"test\")\n",
    "                df_solvs_class[tmp_solv].loc[mask, \"saved_s_best\"] = 0 if a[\"saved_heur\"].sum()<=0 else b[\"saved_heur\"]\n",
    "                df_solvs_class[tmp_solv].loc[mask, \"saved_s_best\"] = b[\"saved_class\"] - df_solvs_class[tmp_solv].loc[mask, \"saved_s_best\"]\n",
    "        elif setting == \"instance\":\n",
    "            p_list = set([prob[0] for prob in df_solvs_class[tmp_solv][\"Problem\"].str.split(\"_\")])\n",
    "            prob_sum = dict()\n",
    "            saved_heur = df_solvs_class[tmp_solv][df_solvs_class[tmp_solv][\"train_test\"]=='train'].sample(frac=0.7).sum()[\"saved_heur\"]\n",
    "            for prob in p_list:\n",
    "                prob_sum[prob] = df_solvs_class[tmp_solv][df_solvs_class[tmp_solv][\"Problem\"].str.contains(prob+\"_\")].query(\"train_test=='test'\").mean(numeric_only=True)\n",
    "            df_solvs_class[tmp_solv] = pd.DataFrame(prob_sum).transpose().drop([\"prediction\"], axis=1)\n",
    "            df_solvs_class[tmp_solv] = df_solvs_class[tmp_solv].reset_index()\n",
    "            df_solvs_class[tmp_solv][\"Problem\"] = df_solvs_class[tmp_solv][\"index\"]\n",
    "            df_solvs_class[tmp_solv] = df_solvs_class[tmp_solv].drop([\"index\"], axis=1)\n",
    "            df_solvs_class[tmp_solv][\"saved_s_best\"] = 0 if saved_heur<=0 else df_solvs_class[tmp_solv][\"saved_heur\"]\n",
    "            df_solvs_class[tmp_solv][\"saved_s_best\"] = df_solvs_class[tmp_solv][\"saved_class\"] - df_solvs_class[tmp_solv][\"saved_s_best\"]\n",
    "    \n",
    "        if \"prediction\" in df_solvs_class[tmp_solv].columns:\n",
    "            df_list.append(df_solvs_class[tmp_solv].query(\"train_test=='test'\").drop([\"prediction\", \"train_test\"], axis=1).copy().assign(Trial=tmp_solv))\n",
    "        elif \"train_test\" in df_solvs_class[tmp_solv].columns:\n",
    "            df_list.append(df_solvs_class[tmp_solv].query(\"train_test=='test'\").drop([\"train_test\"], axis=1).copy().assign(Trial=tmp_solv))\n",
    "        else:\n",
    "            df_list.append(df_solvs_class[tmp_solv].copy().assign(Trial=tmp_solv))\n",
    "\n",
    "        if setting == \"instance\":\n",
    "            df_list[-1] = df_list[-1].copy().assign(Trial=tmp_solv)\n",
    "            cols = [x for x in df_list[-1].columns if x != \"Trial\" and x!= \"Problem\"]\n",
    "            df_list[-1][cols] = df_list[-1][cols].astype(float)\n",
    "    \n",
    "        tmp_solv_name = tmp_solv[0].upper()+tmp_solv[1:]\n",
    "        df_list[-1][\"saved_class - \"+tmp_solv_name] = df_list[-1][\"saved_class\"]\n",
    "        df_list[-1][\"saved_class_v_heur - \"+tmp_solv_name] = df_list[-1][\"saved_class_v_heur\"]\n",
    "        df_list[-1][\"saved_s_best - \"+tmp_solv_name] = df_list[-1][\"saved_s_best\"]\n",
    "        df_list[-1][\"saved_heur - \"+tmp_solv_name] = df_list[-1][\"saved_heur\"]\n",
    "    \n",
    "        df_list[-1] = df_list[-1][[\"saved_class - \"+tmp_solv_name, \"saved_class_v_heur - \"+tmp_solv_name, \n",
    "                                   \"saved_s_best - \"+tmp_solv_name, \"Problem\", \"saved_heur - \"+tmp_solv_name]]\n",
    "    \n",
    "    dfs_boxs_tmp = df_list[0].merge(df_list[1], on=\"Problem\")\n",
    "    dfs_boxs_tmp = dfs_boxs_tmp.merge(df_list[2], on=\"Problem\")\n",
    "    dfs_boxs_tmp = dfs_boxs_tmp.merge(df_list[3], on=\"Problem\")\n",
    "    dfs_boxs_tmp = dfs_boxs_tmp.drop([\"Problem\"], axis=1)\n",
    "    \n",
    "    dfs_boxs_tmp[\"Setting\"] = \" \".join([x[0].upper()+x[1:] for x in setting.split(\"_\")])\n",
    "    dfs_boxs_tmp[\"Setting\"] = dfs_boxs_tmp[\"Setting\"].apply(lambda x: \"By Instance\" if x==\"Instance\" else x)\n",
    "    dfs_boxs_tmp[\"Setting\"] = dfs_boxs_tmp[\"Setting\"].apply(lambda x: \"Per Problem Class\" if x==\"Single Problem\" else x)\n",
    "    \n",
    "    dfs_boxs = pd.concat([dfs_boxs, dfs_boxs_tmp])\n",
    "    \n",
    "    print(\"Finished \" + setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rc('font', size=12) #controls default text size\n",
    "plt.rc('legend', fontsize=10) #fontsize of the legend\n",
    "meanprops = {\"marker\":\"^\",\"markerfacecolor\":\"green\", \"markeredgecolor\":\"white\"}\n",
    "    \n",
    "a = dfs_boxs[[\"Setting\"]+[\"saved_class_v_heur - \"+tmp_solv for tmp_solv in [\"Minion\", \"Kissat\", \"Kissat_mdd\", \"Chuffed\"]]].copy()\n",
    "for c in a.columns:\n",
    "    if c != \"Setting\":\n",
    "        a[c.split(\" - \")[1]] = a[c]\n",
    "        a = a.drop([c], axis=1)\n",
    "a = a.reset_index()\n",
    "a = a.drop([\"index\"], axis=1)\n",
    "\n",
    "mdf = pd.melt(a, id_vars=['Setting'], var_name=['Method'])\n",
    "\n",
    "if len(mdf) > 1:\n",
    "    sns.boxplot(x='Setting', y=\"value\", hue=\"Method\", data=mdf, \n",
    "                hue_order=sorted(mdf[\"Method\"].unique()), showmeans=True, \n",
    "                showfliers = False, meanprops=meanprops);\n",
    "\n",
    "    plt.title(\"Random Forest classifier\");\n",
    "    plt.ylabel(\"Saved time (s)\");\n",
    "    plt.tight_layout();\n",
    "    \n",
    "    #plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.axhline(0, color='black', linestyle='dotted')\n",
    "    #plt.savefig(\"saved_v_heur_settings_boxplot.jpg\", bbox_inches = 'tight');\n",
    "    plt.show()\n",
    "    \n",
    "a = dfs_boxs[[\"Setting\"]+[\"saved_class - \"+tmp_solv for tmp_solv in [\"Minion\", \"Kissat\", \"Kissat_mdd\", \"Chuffed\"]]].copy()\n",
    "for c in a.columns:\n",
    "    if c != \"Setting\":\n",
    "        a[c.split(\" - \")[1]] = a[c]\n",
    "        a = a.drop([c], axis=1)\n",
    "a = a.reset_index()\n",
    "a = a.drop([\"index\"], axis=1)\n",
    "\n",
    "mdf = pd.melt(a, id_vars=['Setting'], var_name=['Method'])\n",
    "\n",
    "if len(mdf) > 1:\n",
    "    sns.boxplot(x='Setting', y=\"value\", hue=\"Method\", data=mdf, \n",
    "                hue_order=sorted(mdf[\"Method\"].unique()), showmeans=True, \n",
    "                showfliers = False, meanprops=meanprops);\n",
    "\n",
    "    plt.title(\"Never using the heuristics\");\n",
    "    plt.ylabel(\"Saved time (s)\");\n",
    "    plt.tight_layout();\n",
    "    #plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.axhline(0, color='black', linestyle='dotted')\n",
    "    #plt.savefig(\"saved_settings_boxplot.jpg\", bbox_inches = 'tight');\n",
    "    plt.show()\n",
    "             \n",
    "a = dfs_boxs[[\"Setting\"]+[\"saved_s_best - \"+tmp_solv for tmp_solv in [\"Minion\", \"Kissat\", \"Kissat_mdd\", \"Chuffed\"]]].copy()\n",
    "for c in a.columns:\n",
    "    if c != \"Setting\":\n",
    "        a[c.split(\" - \")[1]] = a[c]\n",
    "        a = a.drop([c], axis=1)\n",
    "a = a.reset_index()\n",
    "a = a.drop([\"index\"], axis=1)\n",
    "\n",
    "mdf = pd.melt(a, id_vars=['Setting'], var_name=['Method'])\n",
    "\n",
    "if len(mdf) > 1:\n",
    "    sns.boxplot(x='Setting', y=\"value\", hue=\"Method\", data=mdf, \n",
    "                hue_order=sorted(mdf[\"Method\"].unique()), \n",
    "                showmeans=True, showfliers = False, meanprops=meanprops);\n",
    "\n",
    "    plt.title(\"Saved time with respect to the single best\");\n",
    "    plt.ylabel(\"Saved time (s)\");\n",
    "    plt.tight_layout();\n",
    "    #plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.axhline(0, color='black', linestyle='dotted')\n",
    "    #plt.savefig(\"saved_s_best_settings_boxplot.jpg\", bbox_inches = 'tight');\n",
    "    plt.show()\n",
    "    \n",
    "a = dfs_boxs.copy()\n",
    "for tmp_solv in [\"Minion\", \"Kissat\", \"Kissat_mdd\", \"Chuffed\"]:\n",
    "    a[\"saved_s_best - \"+tmp_solv] = a[\"saved_class - \"+tmp_solv]-a[\"saved_s_best - \"+tmp_solv]\n",
    "    a[\"saved_s_best - \"+tmp_solv] = a[\"saved_s_best - \"+tmp_solv]-a[\"saved_heur - \"+tmp_solv]\n",
    "a = a[[\"Setting\"]+[\"saved_s_best - \"+tmp_solv for tmp_solv in [\"Minion\", \"Kissat\", \"Kissat_mdd\", \"Chuffed\"]]].copy()\n",
    "for c in a.columns:\n",
    "    if c != \"Setting\":\n",
    "        a[c.split(\" - \")[1]] = a[c]\n",
    "        a = a.drop([c], axis=1)\n",
    "a = a.reset_index()\n",
    "a = a.drop([\"index\"], axis=1)\n",
    "\n",
    "mdf = pd.melt(a, id_vars=['Setting'], var_name=['Method'])\n",
    "\n",
    "if len(mdf) > 1:\n",
    "    sns.boxplot(x='Setting', y=\"value\", hue=\"Method\", data=mdf, \n",
    "                hue_order=sorted(mdf[\"Method\"].unique()), \n",
    "                showmeans=True, showfliers = False, meanprops=meanprops);\n",
    "\n",
    "    plt.title(\"Single best classifier\");\n",
    "    plt.ylabel(\"Saved time (s)\");\n",
    "    plt.tight_layout();\n",
    "    #plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.axhline(0, color='black', linestyle='dotted')\n",
    "    #plt.savefig(\"saved_s_best_settings_boxplot.jpg\", bbox_inches = 'tight');\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
